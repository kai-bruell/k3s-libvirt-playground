# =============================================================================
# TERRAFORM OUTPUTS
# =============================================================================
# These outputs provide convenient access to cluster information after deployment.
# Run 'terraform output' to display all values, or 'terraform output <name>' for specific values.
# Example: terraform output ssh_control_plane

# -----------------------------------------------------------------------------
# CLUSTER ACCESS INFORMATION
# -----------------------------------------------------------------------------

output "control_plane_ip" {
  description = "IP address of K3s control plane"
  value       = var.control_plane_ip
  # This is the static IP assigned to the control plane node
  # Used for SSH access and Kubernetes API endpoint (port 6443)
}

output "worker_ips" {
  description = "IP addresses of K3s worker nodes"
  value       = [for i in range(var.worker_count) : cidrhost("192.168.56.0/24", 11 + i)]
  # Worker IPs are calculated dynamically based on worker_count
  # cidrhost() generates sequential IPs: 192.168.56.11, .12, .13, etc.
  # Returns empty list if worker_count = 0
}

# -----------------------------------------------------------------------------
# SSH ACCESS COMMANDS
# -----------------------------------------------------------------------------
# These outputs provide ready-to-use SSH commands for cluster access.
# The SSH key was auto-generated by Terraform (see tls_private_key resource).
# StrictHostKeyChecking=no skips host key verification (OK for local VMs).

output "ssh_private_key_path" {
  description = "Path to generated SSH private key"
  value       = abspath("${path.module}/../k3s_cluster_id_rsa")
  # This key is created by the local_sensitive_file resource in main.tf
  # File permissions are set to 0600 (read/write owner only)
}

output "ssh_control_plane" {
  description = "SSH command for control plane access"
  value       = "ssh -i ${abspath(path.module)}/../k3s_cluster_id_rsa -o StrictHostKeyChecking=no debian@${var.control_plane_ip}"
  # Copy this command and run it directly in your terminal
  # The cloud-init image uses 'debian' as the default user
}

output "ssh_workers" {
  description = "SSH commands for worker node access"
  value = {
    for i in range(var.worker_count) : "worker-${i + 1}" =>
    "ssh -i ${abspath(path.module)}/../k3s_cluster_id_rsa -o StrictHostKeyChecking=no debian@${cidrhost("192.168.56.0/24", 11 + i)}"
  }
  # Returns a map: { "worker-1" = "ssh ...", "worker-2" = "ssh ...", ... }
  # Access specific worker: terraform output -json ssh_workers | jq -r '.["worker-1"]'
}

# -----------------------------------------------------------------------------
# KUBERNETES ACCESS
# -----------------------------------------------------------------------------
# K3s automatically generates a kubeconfig at /etc/rancher/k3s/k3s.yaml
# These outputs help you retrieve and configure kubectl access to the cluster.

output "kubeconfig_command" {
  description = "Command to retrieve kubeconfig from control plane"
  value       = "ssh -i ${abspath(path.module)}/../k3s_cluster_id_rsa -o StrictHostKeyChecking=no debian@${var.control_plane_ip} 'sudo cat /etc/rancher/k3s/k3s.yaml'"
  # This fetches the kubeconfig file from the control plane
  # The file contains certificates and cluster endpoint information
  # Requires sudo because k3s.yaml has restricted permissions (0600, root-owned)
}

output "kubeconfig_setup" {
  description = "Complete setup instructions for kubeconfig"
  value       = <<-EOT
    # Retrieve kubeconfig:
    ssh -i ${abspath(path.module)}/../k3s_cluster_id_rsa -o StrictHostKeyChecking=no debian@${var.control_plane_ip} 'sudo cat /etc/rancher/k3s/k3s.yaml' > k3s.yaml

    # Update server IP:
    sed -i 's/127.0.0.1/${var.control_plane_ip}/g' k3s.yaml

    # Use kubeconfig:
    export KUBECONFIG=$(pwd)/k3s.yaml
    kubectl get nodes
  EOT
  # Step-by-step instructions for kubectl setup:
  # 1. Download kubeconfig from control plane
  # 2. Replace localhost (127.0.0.1) with actual control plane IP
  #    (K3s defaults to localhost in the generated config)
  # 3. Set KUBECONFIG environment variable to use this config
  # 4. Verify cluster access with 'kubectl get nodes'
}

output "kubernetes_api_endpoint" {
  description = "Kubernetes API server endpoint"
  value       = "https://${var.control_plane_ip}:6443"
  # The Kubernetes API server listens on port 6443 (K3s default)
  # This is the endpoint kubectl will connect to
  # Must be reachable from your machine (NAT network provides access)
}

# -----------------------------------------------------------------------------
# CLUSTER INFORMATION
# -----------------------------------------------------------------------------
# Structured summary of the entire cluster configuration.
# Useful for scripts, automation, or quick cluster overview.

output "cluster_info" {
  description = "K3s cluster summary"
  value = {
    control_plane = {
      ip     = var.control_plane_ip
      name   = "k3s-control-plane"
      memory = var.control_plane_memory # MB
      vcpus  = var.control_plane_vcpus
    }
    workers = {
      count  = var.worker_count
      memory = var.worker_memory # MB per worker
      vcpus  = var.worker_vcpus  # vCPUs per worker
      ips    = [for i in range(var.worker_count) : cidrhost("192.168.56.0/24", 11 + i)]
    }
  }
  # This output provides a complete overview of cluster resources
  # View as JSON: terraform output -json cluster_info | jq
  # Useful for documentation or monitoring scripts
}

# -----------------------------------------------------------------------------
# DEBUG INFORMATION
# -----------------------------------------------------------------------------
# Advanced outputs for troubleshooting VM boot and cloud-init issues.

output "cloudinit_iso_locations" {
  description = "Locations of cloud-init ISO files for debugging"
  value = concat(
    ["${abspath(path.module)}/../libvirt-pool/k3s-control-plane-cloudinit.iso"],
    [for i in range(var.worker_count) : "${abspath(path.module)}/../libvirt-pool/k3s-worker-${i + 1}-cloudinit.iso"]
  )
  # Cloud-init ISOs contain the user-data, network-config, and meta-data
  # If VMs fail to boot or configure properly, you can:
  # 1. Mount these ISOs to inspect their contents
  # 2. Check for syntax errors in the cloud-init YAML
  # 3. Verify SSH keys and network configuration
  # Example: 7z x <iso-file> -o/tmp/cloudinit-debug
}
